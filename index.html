<!DOCTYPE html>
<html lang="en">

  <head>

    <meta charset="utf-8">

    <title>Xingyi Zhou</title>

    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="css/style.css" rel="stylesheet">
  </head>

  <body>

    <div class="container col-xs-12 col-sm-8 col-md-8 col-lg-8 col-md-offset-2 col-lg-offset-2 col-sm-offset-2 p-0">

        <h1 id = "myName">Xingyi 
            <span class="text-primary">Zhou</span>
        </h1>

        <ul class="nav nav-tabs" id="myTab" role="tablist">
        <li class="nav-item">
          <a class="nav-link active" id="home-tab" data-toggle="tab" href="#home" role="tab" aria-controls="home" aria-selected="true">Home</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" id="publication-tab" data-toggle="tab" href="#publication" role="tab" aria-controls="publication" aria-selected="false">Research</a>
        </li>
      </ul>
      <div class="tab-content" id="nav-tabContent">
    <div class="tab-pane fade show active" id="home" role="tabpanel" aria-labelledby="home-tab">

        <table>
          <tr>
            <td width = "30%">
              <img src="assets/photo.jpeg" alt="" align="center" style="width: 50%;">
            </td>
              <td width = "1%">
            </td>
            <td width = "62%">
              <p align>I am a Research Scientist at Google Research. I did my Ph.D. in Computer Science at The University of Texas at Austin, supervised by Prof. <a href="http://www.philkr.net/">Philipp Kr&auml;henb&uuml;hl</a>. Before that, I obtained my bachelor degree from School of Computer Science at Fudan University. I have interned at Microsoft Research Asia, Google Research, Intel Labs, and Facebook AI Research. 
			  <p>My research focuses on object-level visual recognition in videos, including object captioning, detection, 3D perception, pose estimation, and tracking.</p>
			  
                <a href="assets/XingyiZhou_CV.pdf" target="_blank">CV</a> / <a href="https://scholar.google.com/citations?user=47n-0mwAAAAJ&hl=en">Google Scholar</a> / <a href="https://github.com/xingyizhou">GitHub</a> / <a href="https://www.linkedin.com/in/xingyi-zhou-21925290/">LinkedIn</a>

            </td>

        </tr>
        
        </table>
        <br>
<!--         <p id = "linkToJiarui">The profile picture is taken by my lovely girlfriend <a href="http://jiaruigao.com" >Jiarui Gao</a>.</p> -->
        <p align="center"> Last updated June 2023</p>
        </div>
        
        <div class="tab-pane fade" id="publication" role="tabpanel" aria-labelledby="publication-tab">
		
          <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <h4 style="margin-top: 10px">Overview</h4>
              <hr>
              <p>
			  Large-scale well-curated datasets are treasures in computer vision. However, most datasets only focus on one single domain with a specific task and a fixed label set. Computer vision models trained on a single dataset can not generalize to all applications in the real world. 
			  <br> <br> 
			  The goal of my research is to remove the artificial barriers of datasets and make object recognition generalize in the wild. There should be one single computer vision model, not a zoo of dataset-specific models. The model should be trained on a diverse set of datasets and should be able to recognize objects from different data sources in all domains. 
			  <br> <br> 
			  Towards this goal, my research focuses on three aspects: 1. How to build a unified object representation for various vision tasks. I am proposing a point-based representation for object detection, 3D detection, and pose estimation (<a href="https://github.com/xingyizhou/CenterNet">CenterNet</a>, <a href="https://github.com/xingyizhou/CenterNet2">CenterNet2</a>, <a href="https://github.com/tianweiy/CenterPoint">CenterPoint</a>). 2. How to generalize object recognition through time. I propose a simple solution to extend our point-based detector into a local tracker (<a href="https://github.com/xingyizhou/CenterTrack">CenterTrack</a>), and then introduce a global tracker that does recognition globally over time (<a href="https://github.com/xingyizhou/GTR">GTR</a>). 3. How to expand the vocabulary of an object detection system. I learn a unified label space for different detection datasets (<a href="https://github.com/xingyizhou/UniDet">UniDet</a>) and combine classification datasets with twenty-thousand classes (<a href="https://github.com/facebookresearch/Detic">Detic</a>).
			  <br> <br> 
			  Going further, I am interested in training a unified object recognition system that performs multi-tasks using weak or incomplete supervisions.
			  </p>
              <br>
			  
		<h4>2022</h4>
		<hr>

          <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <img src="assets/densevoc.png" alt="" align="left" style="width: 60%;display:block;margin-left: auto;margin-right: auto; margin-right: 40%;">
              <br> <br> 
              <papertitle class=blue><a href="https://arxiv.org/abs/2306.11729">Dense Video Object Captioning from Disjoint Supervision</a></papertitle>
              <br><strong>Xingyi Zhou</strong>, Anurag Arnab, Chen Sun, Cordelia Schmid <br>
              <span>arXiv, 2023</span><br>
            </div>
          </div>
          <br>

          <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <img src="assets/objectvivit.png" alt="" align="left" style="width: 30%;display:block;margin-left: auto;margin-right: auto;margin-right: 70%;">
              <br />
              <papertitle class=blue><a href="https://arxiv.org/abs/2306.11726">How can objects help action recognition?</a></papertitle>
              <br><strong>Xingyi Zhou</strong>, Anurag Arnab, Chen Sun, Cordelia Schmid <br>
              <span>CVPR, 2023</span><br>
              <i class=blue><a href="https://github.com/google-research/scenic/tree/main/scenic/projects/objectvivit" target="_blank">code</a>&nbsp/&nbsp</i>
            </div>
          </div>
          <br>

          <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <img src="https://github.com/facebookresearch/Detic/raw/main/docs/teaser.jpeg" alt="" align="left" style="width: 40%;display:block;margin-left: auto;margin-right: auto;margin-right: 60%;">
              <br />
              <papertitle class=blue><a href="https://arxiv.org/abs/2201.02605">Detecting Twenty-thousand Classes using Image-level Supervision</a></papertitle>
              <br><strong>Xingyi Zhou</strong>, Rohit Girdhar, Armand Joulin, Philipp Kr&auml;henb&uuml;hl, Ishan Misra <br>
              <span>ECCV, 2022</span><br>
              <i class=blue><a href="assets/Detic.bib">bibtex</a> &nbsp/&nbsp</i>
              <i class=blue><a href="https://github.com/facebookresearch/Detic" target="_blank">code</a>&nbsp/&nbsp</i>
              <i class=blue><a href="https://github.com/facebookresearch/Detic/blob/main/docs/MODEL_ZOO.md" target="_blank">models</a>&nbsp/&nbsp</i>
              <i class=blue><a href="https://huggingface.co/spaces/akhaliq/Detic" target="_blank">web demo</a></i>
            </div>
          </div>
          <br>
		  
          <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <img src="https://github.com/xingyizhou/GTR/raw/master/docs/GTR_teaser.jpg" alt="" align="left" style="width: 80%;display:block;margin-left: auto;margin-right: auto;margin-right: 20%;">
              <br />
              <papertitle class=blue><a href="https://arxiv.org/abs/2203.13250">Global Tracking Transformers</a></papertitle>
              <br><strong>Xingyi Zhou</strong>, Tianwei Yin, Vladlen Koltun, Philipp Kr&auml;henb&uuml;hl<br>
              <span>CVPR, 2022</span><br>
              <i class=blue><a href="assets/GTR.bib">bibtex</a> &nbsp/&nbsp</i>
              <i class=blue><a href="https://github.com/xingyizhou/GTR" target="_blank">code</a>&nbsp/&nbsp</i>
              <i class=blue><a href="https://github.com/xingyizhou/GTR/docs/MODEL_ZOO.md" target="_blank">models</a></i>
            </div>
          </div>
          <br>

          <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <img src="assets/UniDet.jpg" alt="" align="left" style="width: 70%;display:block;margin-left: auto;margin-right: auto;margin-right: 30%;">
              <br />
              <papertitle class=blue><a href="https://arxiv.org/abs/2102.13086">Simple multi-dataset detection</a></papertitle>
              <br><strong>Xingyi Zhou</strong>, Vladlen Koltun, Philipp Kr&auml;henb&uuml;hl<br>
              <span>CVPR, 2022</span><br>
              <i class=blue><a href="assets/UniDet.bib">bibtex</a> &nbsp/&nbsp</i>
              <i class=blue><a href="https://github.com/xingyizhou/UniDet" target="_blank">code</a>&nbsp/&nbsp</i>
              <i class=blue><a href="https://github.com/xingyizhou/UniDet/blob/master/projects/UniDet/unidet_docs/REPRODUCE.md" target="_blank">models</a></i>
            </div>
          </div>
          <br>
		  
    <h4>2021</h4>
    <hr>

          <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <img src="assets/CenterNet2.jpg" alt="" align="left" style="width: 70%;display:block;margin-left: auto;margin-right: auto;margin-right: 30%;">
              <br />
              <papertitle class=blue><a href="https://arxiv.org/abs/2103.07461">Probabilistic two-stage detection</a></papertitle>
              <br><strong>Xingyi Zhou</strong>, Vladlen Koltun, Philipp Kr&auml;henb&uuml;hl<br>
              <span>arXiv technical report, 2021</span><br>
              <i class=blue><a href="assets/CenterNet2.bib">bibtex</a> &nbsp/&nbsp</i>
              <i class=blue><a href="https://github.com/xingyizhou/CenterNet2" target="_blank">code</a>&nbsp/&nbsp</i>
              <i class=blue><a href="https://github.com/xingyizhou/CenterNet2/blob/master/projects/CenterNet2/centernet2_docs/MODEL_ZOO.md" target="_blank">models</a></i>
            </div>
          </div>
          <br>

          <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <img src="https://github.com/tianweiy/MVP/raw/main/docs/teaser.png" alt="" align="left" style="width: 50%;display:block;margin-left: auto;margin-right: auto;margin-right: 50%;">
              <br />
              <papertitle class=blue><a href="https://arxiv.org/abs/2111.06881">Multimodal Virtual Point 3D Detection</a></papertitle>
              <br>Tianwei Yin, <strong>Xingyi Zhou</strong>, Philipp Kr&auml;henb&uuml;hl<br>
              <span>NeurIPS, 2021</span><br>
              <i class=blue><a href="assets/TianweiYin-neurips2021.bib">bibtex</a> &nbsp/&nbsp</i>
              <i class=blue><a href="https://github.com/tianweiy/MVP" target="_blank">code</a>&nbsp/&nbsp</i>
            </div>
          </div>
          <br>
			  
          <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <img src="assets/CenterPoint.png" alt="" align="left" style="width: 50%;display:block;margin-left: auto;margin-right: auto;margin-right: 50%;">
              <br />
              <papertitle class=blue><a href="https://arxiv.org/abs/2006.11275">Center-based 3D Object Detection and Tracking</a></papertitle>
              <br>Tianwei Yin, <strong>Xingyi Zhou</strong>, Philipp Kr&auml;henb&uuml;hl<br>
              <span>CVPR, 2021</span><br>
              <i class=blue><a href="assets/TianweiYin-cvpr2021.bib">bibtex</a> &nbsp/&nbsp</i>
              <i class=blue><a href="https://github.com/tianweiy/CenterPoint" target="_blank">code</a>&nbsp/&nbsp</i>
              <i class=blue><a href="https://github.com/tianweiy/CenterPoint/blob/master/docs/MODEL_ZOO.md" target="_blank">models</a></i>
            </div>
          </div>
          <br>
			  
        <h4>2020</h4>
        <hr>
			  
          <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <img src="assets/CenterTrack.png" alt="PontTrust" align="left" style="width: 70%;display:block;margin-left: auto;margin-right: auto;margin-right: 30%;">
              <br />
              <papertitle class=blue><a href="http://arxiv.org/abs/2004.01177">Tracking Objects as Points</a></papertitle>
              <br><strong>Xingyi Zhou</strong>, Vladlen Koltun, Philipp Kr&auml;henb&uuml;hl<br>
              <span>ECCV, 2020 (Spotlight)</span><br>
              <i class=blue><a href="assets/XingyiZhou-ECCV2020.bib">bibtex</a> &nbsp/&nbsp</i>
              <i class=blue><a href="https://github.com/xingyizhou/CenterTrack" target="_blank">code</a>&nbsp/&nbsp</i>
              <i class=blue><a href="https://github.com/xingyizhou/CenterTrack/blob/master/readme/MODEL_ZOO.md" target="_blank">model zoo</a></i>
            </div>
          </div>
          <br>
			  
			  
		<h4>2019</h4>
		<hr>
			  
          <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <img src="assets/arXiv2019.jpg" alt="PontTrust" align="left" style="width: 50%;display:block;margin-left: auto;margin-right: auto;margin-right: 50%;">
              <br />
              <papertitle class=blue><a href="https://arxiv.org/abs/1904.07850">Objects as Points</a></papertitle>
              <br><strong>Xingyi Zhou</strong>, Dequan Wang, Philipp Kr&auml;henb&uuml;hl<br>
              <span>arXiv technical report, 2019</span><br>
              <i class=blue><a href="assets/XingyiZhou-arXiv2019.bib">bibtex</a> &nbsp/&nbsp</i>
              <i class=blue><a href="https://github.com/xingyizhou/CenterNet" target="_blank">code</a>&nbsp/&nbsp</i>
              <i class=blue><a href="https://github.com/xingyizhou/CenterNet/blob/master/readme/MODEL_ZOO.md" target="_blank">model zoo</a></i>
            </div>
          </div>
          <br>

          <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <img src="assets/CVPR2019.jpg" alt="PontTrust" align="left" style="width: 50%;display:block;margin-left: auto;margin-right: auto;margin-right: 50%;">
              <br />
              <papertitle class=blue><a href="https://arxiv.org/abs/1901.08043">Bottom-up Object Detection by Grouping Extreme and Center Points</a></papertitle>
              <br><strong>Xingyi Zhou</strong>, Jiacheng Zhuo, Philipp Kr&auml;henb&uuml;hl<br>
              <span>Computer Vision and Pattern Recognition (CVPR), 2019</span><br>
              <i class=blue><a href="assets/XingyiZhou-CVPR2019.bib">bibtex</a> &nbsp/&nbsp</i>
              <i class=blue><a href="https://github.com/xingyizhou/ExtremeNet" target="_blank">code</a>&nbsp/&nbsp</i>
              <i class=blue><a href="https://drive.google.com/open?id=1re-A74WRvuhE528X6sWsg1eEbMG8dmE4" target="_blank">model</a>&nbsp/&nbsp</i>
              <i class=blue><a href="https://drive.google.com/open?id=1te-CHOVpW550TuCHUYQQyOHzEKPHkGXe" target="_blank">supplementary</a></i>
            </div>
          </div>
          <br>
          

          <h4>2018</h4>
              <hr>
          <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <img src="assets/arXiv2018.jpg" alt="PontTrust" align="left" style="width: 50%;display:block;margin-left: auto;margin-right: auto;margin-right: 50%;">
              <br />
              <papertitle class=blue><a href="https://arxiv.org/abs/1803.09331">StarMap for Category-Agnostic Keypoint and Viewpoint Estimation</a></papertitle>
              <br><strong>Xingyi Zhou</strong>, Arjun Karpur, Linjie Luo, Qixing Huang<br>
              <span>European Conference on Computer Vision (ECCV), 2018</span><br>
              <i class=blue><a href="assets/XingyiZhou-arXiv2018.bib">bibtex</a> &nbsp/&nbsp</i>
              <i class=blue><a href="https://github.com/xingyizhou/StarMap" target="_blank">code</a>&nbsp/&nbsp</i>
              <i class=blue><a href="https://drive.google.com/file/d/1bwCeC4F0OLFYceiaAuUGB6pU8OOZor1k/view?usp=sharing" target="_blank">model</a>&nbsp/&nbsp</i>
              <i class=blue><a href="https://drive.google.com/file/d/1IEcHBdQ8u2HTKiNz88ItJWDRQUOJnf60/view?usp=sharing" target="_blank">supplementary</a>&nbsp/&nbsp</i>
              <i class=blue><a href="assets/ECCV18_starmap_poster.pdf" target="_blank">poster</a></i>
            </div>
          </div>
          <br>

          <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <img src="assets/arXiv2017_2.png" alt="PontTrust" align="left" style="width: 50%;display:block;margin-left: auto;margin-right: auto;margin-right: 50%;">
              <br />
              <papertitle class=blue><a href="https://arxiv.org/abs/1712.05765">  Unsupervised Domain Adaptation for 3D Keypoint Estimation via View Consistency</a></papertitle>
              <br><strong>Xingyi Zhou</strong>, Arjun Karpur, Chuang Gan, Linjie Luo, Qixing Huang<br>
              <span>European Conference on Computer Vision (ECCV), 2018</span><br>
              <i class=blue><a href="assets/XingyiZhou-arXiv2017-2.bib">bibtex</a> &nbsp/&nbsp</i>
              <i class=blue><a href="https://github.com/xingyizhou/3DKeypoints-DA" target="_blank">code</a>&nbsp/&nbsp</i>
              <i class=blue><a href="https://drive.google.com/file/d/1nXNPHr8UffI79yT0fBPOy-mTb5iqoYe6/view" target="_blank">model</a>&nbsp/&nbsp</i>
              <i class=blue><a href="assets/ECCV18_da_poster.pdf" target="_blank">poster</a></i>
            </div>
          </div>
          <br>
          <h4>2017</h4>
              <hr>

          <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <img src="assets/XingyiZhou-ICCV2017.jpg" alt="PontTrust" align="left" style="width: 70%;display:block;margin-left: auto;margin-right: auto;margin-right: 30%;">
              <br />
              <papertitle class=blue><a href="https://arxiv.org/abs/1704.02447">Towards 3D Human Pose Estimation in the Wild: A weakly-supervised Approach</a></papertitle>
              <br><strong>Xingyi Zhou</strong>, Qixing Huang, Xiao Sun, Xiangyang Xue, Yichen Wei<br>
              <span>International Conference on Computer Vision (ICCV), 2017</span><br>
              <i class=blue><a href="assets/XingyiZhou-arXiv2017.bib">bibtex</a> &nbsp/&nbsp</i>
              <i class=blue><a href="https://github.com/xingyizhou/pose-hg-3d" target="_blank">code (torch)</a>&nbsp/&nbsp</i>
              <i class=blue><a href="https://github.com/xingyizhou/Pytorch-pose-hg-3d" target="_blank">code (PyTorch)</a>&nbsp/&nbsp</i>
              <i class=blue><a href="http://xingyizhou.xyz/hgreg-3d.t7" target="_blank">model</a>&nbsp/&nbsp</i>
              <i class=blue><a href="assets/XingyiZhou-arXiv2017-Supplementary.pdf" target="_blank">supplementary</a>&nbsp/&nbsp</i>
              <i class=blue><a href="assets/ICCV17-poster.pdf" target="_blank">poster</a></i>
            </div>
          </div>
          <br>
          <h4>2016</h4>
              <hr>

          <div class="resume-item d-flex flex-column flex-md-row ">
            <div class="resume-content mr-auto">
              <img src="assets/XingyiZhou-GMDL2016.png" alt="PontTrust" align="left" style="width: 60%;display:block;margin-left: auto;margin-right: auto;margin-right: 40%;">
              <br />
              <papertitle class=blue><a href="http://arxiv.org/abs/1609.05317">Deep Kinematic Pose Regression</a></papertitle>
              <br><strong>Xingyi Zhou</strong>, Xiao Sun, Wei Zhang, Shuang Liang, Yichen Wei<br>
              <span>ECCV Workshop on Geometry Meets Deep Learning, 2016</span><br>
              <i class=blue><a href="assets/XingyiZhou-GMDL2016.bib">bibtex</a> &nbsp/&nbsp</i>
              <i class=blue><a href="assets/ECCVW16/ECCVW16_src.zip">code</a>&nbsp/&nbsp</i>
              <i class=blue><a href="assets/ECCVW16/Model.caffemodel">model</a>&nbsp/&nbsp</i>
              <i class=blue><a href="assets/XingyiZhou-GMDL2016-Poster.pdf" target="_blank">poster</a></i>
            </div>
          </div>
          <br>

          <div class="resume-item d-flex flex-column flex-md-row ">
            <div class="resume-content mr-auto">
              <img src="assets/XingyiZhou-IJCAI2016.png" alt="PontTrust" align="left" style="width: 60%;display:block;margin-left: auto;margin-right: auto;margin-right: 40%;">
              <br />
              <papertitle class=blue><a href="https://arxiv.org/abs/1606.06854">Model-based Deep Hand Pose Estimation</a></papertitle>
              <br><strong>Xingyi Zhou</strong>, Qingfu Wan, Wei Zhang, Xiangyang Xue, Yichen Wei<br>
              <span>International Joint Conference on Artificial Intelligence  (IJCAI), 2016</span><br>
              <i class=blue><a href="assets/XingyiZhou-IJCAI2016.bib">bibtex</a> &nbsp/&nbsp</i>
              <i class=blue><a href="https://github.com/xingyizhou/DeepModel" target="_blank">code</a>&nbsp/&nbsp</i>
              <i class=blue><a href="assets/XingyiZhou-IJCAI2016-Slides.pdf" target="_blank">slides</a>&nbsp/&nbsp</i>
              <i class=blue><a href="assets/XingyiZhou-IJCAI2016-Poster.pdf" target="_blank">poster</a></i>
              
            </div>
          </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
    <br>
    <br>

    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

  </body>

</html>
