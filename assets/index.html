<!--<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">-->
<!DOCTYPE html>
<html>

<head>
    <meta name=viewport content='width=800'>
    <link rel="shortcut icon" href="FDU.ico">
    <script type="text/javascript" src="hidebib.js"></script>
    <style type="text/css">
        a {
            color: #1772d0;
            text-decoration: none;
        }
        
        a:focus,
        a:hover {
            color: #f09228;
            text-decoration: none;
        }
        
        body,
        td,
        th {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px
        }
        
        strong {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px;
        }
        
        heading {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 18px;
        }
        
        papertitle {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 15px;
            font-weight: 700
        }
        
        name {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 32px;
        }
        
        .fade {
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }
    </style>
    <!--    <link rel="icon" type="image/png" href="seal_icon.png">-->
    <title>Xingyi Zhou</title>
    <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
    <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>

    <script type="text/javascript">
        window._pt_lt = new Date().getTime();
        window._pt_sp_2 = [];
        _pt_sp_2.push('setAccount,6fc2acb1');
        var _protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
        (function () {
            var atag = document.createElement('script');
            atag.type = 'text/javascript';
            atag.async = true;
            atag.src = _protocol + 'js.ptengine.com/pta.js';
            var stag = document.createElement('script');
            stag.type = 'text/javascript';
            stag.async = true;
            stag.src = _protocol + 'js.ptengine.com/pts.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(atag, s);
            s.parentNode.insertBefore(stag, s);
        })();
    </script>

    <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
        <tr>
            <td>
                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td width="67%" valign="middle">
                            <p align="center">
                                <name>Xingyi Zhou</name>
                                </font>
                                <p align>I am a fresh Computer Science Ph.D. student at The University of Texas at Austin, advised by Prof. <a href="http://www.cs.utexas.edu/~huangqx/">Qixing Huang</a>. Before coming to Austin, I graduated from School of Computer Science at Fudan University, advised by Prof. <a href="http://weizh2016.github.io/">Wei Zhang</a> and Prof. <a href="https://scholar.google.com.hk/citations?user=DTbhX6oAAAAJ">Xiangyang Xue</a>. I have spent 6 months as a research intern at Microsoft Research Asia, working with Dr. <a href="https://www.microsoft.com/en-us/research/people/yichenw/">Yichen Wei</a>.
                                    <br>
                                    <p align=center>
                                        <a href="mailto:zhouxy13@fudan.edu.cn">Email</a> &nbsp/&nbsp
                                        <a href="XingyiZhou-CV.pdf" target="_blank">CV</a> &nbsp/&nbsp
                                        <a href="https://github.com/xingyizhou" target="_blank">Github</a> &nbsp/&nbsp
                                        <a href="https://scholar.google.com/citations?user=47n-0mwAAAAJ&hl=en">Google Scholar</a>
                                    </p>

                        </td>
                        <td width="33%">
                            <img src="XingyiZhou-Im.jpg" width="180" />

                        </td>
                    </tr>
                </table>
                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td>
                            <heading>Publication</heading>
                            <p>My research focuses on computer vision and machine learning, especially for object keypoints estimation.</p>
                        </td>
                    </tr>
                </table>

                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td width="100%"><img src="XingyiZhou-ICCV2017.jpg" alt="PontTrust" width="700" style="border-style: none"></td>
                    </tr>
                    <tr>
                        <td valign="top" width="100%">
                            <p>
                                <a href="https://arxiv.org/abs/1704.02447" target="_blank">
                                    <papertitle>Towards 3D Human Pose Estimation in the Wild: A weakly-supervised Approach</papertitle>
                                </a>
                                <br>
                                <strong>Xingyi Zhou</strong>, Qixing Huang, Xiao Sun, Xiangyang Xue, Yichen Wei
                                <br>
                                <em>International Conference on Computer Vision (ICCV)</em>, 2017
                                <br>
                                <a href="XingyiZhou-arXiv2017.bib">bibtex</a> &nbsp/&nbsp
                                <a href="https://github.com/xingyizhou/pose-hg-3d" target="_blank">code</a>&nbsp/&nbsp
                                <a href="http://xingyizhou.xyz/hgreg-3d.t7" target="_blank">model</a>&nbsp/&nbsp
                                <a href="XingyiZhou-arXiv2017-Supplementary.pdf" target="_blank">supplementary</a>&nbsp/&nbsp
                                <a href="ICCV17-poster.pdf" target="_blank">poster</a>

                                <br>
                                <p></p>
                                <p>
                                    We propose a weakly-supervised transfer learning method that learns an end-to-end network using training data with mixed 2D and 3D labels. The network augments a state-of-the-art 2D pose estimation network with a 3D depth regression network. The 3D pose labels in controlled environments are transferred to images in the wild that only possess 2D annotations. Importantly, we introduce a 3D geometric constraint to regularize the prediction 3D poses, which is effective on images that only have 2D annotations.
                                <p>
                                    </p>
                                    </a>
                                </p>
                                <pre xml:space="preserve" id="arXiv2017">
            </pre>
                        </td>
                    </tr>




                    <tr>
                        <td width="100%"><img src="XingyiZhou-GMDL2016.png" alt="PontTrust" width="700" style="border-style: none"></td>
                    </tr>
                    <tr>
                        <td valign="top" width="100%">
                            <p>
                                <a href="http://arxiv.org/abs/1609.05317" target="_blank">
                                    <papertitle>Deep Kinematic Pose Regression</papertitle>
                                </a>
                                <br>
                                <strong>Xingyi Zhou</strong>, Xiao Sun, Wei Zhang, Shuang Liang, Yichen Wei
                                <br>
                                <em>ECCV Workshop on Geometry Meets Deep Learning</em>, 2016
                                <br>
                                <!--                                <a href="#">project page</a> /-->
                                <!-- <a href="DequanWang-ICCV2015-Abstract.pdf">abstract</a> / -->
                                <a href="XingyiZhou-GMDL2016.bib">bibtex</a> &nbsp/&nbsp
                                <!-- <a href="https://github.com/tenstep/DeepModel" target="_blank">code</a>&nbsp/&nbsp -->
                                <!-- <a href="XingyiZhou-IJCAI2016-Slides.pdf" target="_blank">slides</a>&nbsp/&nbsp -->
                                <a href="XingyiZhou-GMDL2016-Poster.pdf" target="_blank">poster</a>
                                <!-- <a href="DequanWang-ICCV2015.key">keynote</a> / -->
                                <!-- <a href="http://www.youtube.com/watch?v=*">video</a>-->
                                <!-- <a href="DequanWang-ICCV2015.zip">code</a>-->
                                <p></p>
                                <p> We propose to directly embed a kinematic object model into the deep neutral network learning for general articulated object pose estimation. The kinematic function is defined on the appropriately parameterized object motion variables. We show convincing experiment results on a toy example, and we achieve state-of-the-art result on Human3.6M dataset for the 3D human pose estimation problem.
                                    <p></p>
                                    </a>
                                </p>
                                <pre xml:space="preserve" id="IJCAI2016">
            </pre>
                        </td>
                    </tr>

                    <br>
                    <br>



                    <tr>
                        <td width="100%"><img src="XingyiZhou-IJCAI2016.png" alt="PontTrust" width="700" style="border-style: none"></td>
                    </tr>
                    <tr>
                        <td valign="top" width="100%">
                            <p>
                                <a href="zhou2016model.pdf" target="_blank">
                                    <papertitle>Model-based Deep Hand Pose Estimation</papertitle>
                                </a>
                                <br>
                                <strong>Xingyi Zhou</strong>, Qingfu Wan, Wei Zhang, Xiangyang Xue, Yichen Wei
                                <br>
                                <em>International Joint Conference on Artificial Intelligence  (IJCAI)</em>, 2016
                                <br>
                                <a href="XingyiZhou-IJCAI2016.bib">bibtex</a> &nbsp/&nbsp
                                <a href="https://github.com/tenstep/DeepModel" target="_blank">code</a>&nbsp/&nbsp
                                <a href="XingyiZhou-IJCAI2016-Slides.pdf" target="_blank">slides</a>&nbsp/&nbsp
                                <a href="XingyiZhou-IJCAI2016-Poster.pdf" target="_blank">poster</a>
                                <br>
                                <p></p>
                                <p>
                                    We propose a model based deep learning approach that adopts a forward kinematics based layer to ensure the geometric validity of estimated poses. For the first time, we show that embedding such a non-linear generative process in deep learning is feasible for hand pose estimation.
                                <p>
                                    </p>
                                    </a>
                                </p>
                                <pre xml:space="preserve" id="IJCAI2016">
            </pre>
                        </td>
                    </tr>

                </table>

                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td>
                            <heading>Selected Awards</heading>
                        </td>
                    </tr>
                </table>
                <table width="100%" align="center" border="0" cellpadding="20">

                    <tr>
                        <td width="20%"><img src="Icpc_logo.png" alt="PontTrust" width="100" style="border-style: none"></td>
                        <td width="80%" valign="top">
                            <p>
                                <papertitle>ACM International Collegiate Programming Contest, Asia Regional,</papertitle>
                                <br>
                                <papertitle>Gold Medal</papertitle>
                                <br> Shanghai Site, 2014
                                <br>
                                
                                </a>
                            </p>
                        </td>
                    </tr>

                    <tr>
                        <td width="20%"><img src="StarOfTomorrow-logo.png" alt="PontTrust" width="100" style="border-style: none"></td>
                        <td width="80%" valign="top">
                            <p>
                                <papertitle>Award of Excellence for Stars of Tomorrow Internship Program</papertitle>
                                <br> Microsoft Research Asia, 2016
                                <br>
                                
                                </a>
                            </p>
                        </td>
                    </tr>

                </table>
                </font>
                </p>
            </td>
        </tr>
    </table>
    <script type="text/javascript">
        var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
        document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
    </script>
    <script type="text/javascript">
        try {
            var pageTracker = _gat._getTracker("UA-67905219-1");
            pageTracker._trackPageview();
        } catch (err) {}
    </script>
    </td>
    </tr>
    </table>
</body>

</html>